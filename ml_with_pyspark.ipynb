{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget -P data\\\n",
        "    https://raw.githubusercontent.com/Renua-Meireles/ML_with_pyspark/main/data/clustering_dataset.csv\\\n",
        "    https://raw.githubusercontent.com/Renua-Meireles/ML_with_pyspark/main/data/employee.txt\\\n",
        "    https://raw.githubusercontent.com/Renua-Meireles/ML_with_pyspark/main/data/iris.csv\\\n",
        "    https://raw.githubusercontent.com/Renua-Meireles/ML_with_pyspark/main/data/winequality-red.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_spyXXBMCiT",
        "outputId": "d3b3c1b3-222d-4a5e-81c7-19d588b2a325"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-28 20:32:31--  https://raw.githubusercontent.com/Renua-Meireles/ML_with_pyspark/main/data/clustering_dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 619 [text/plain]\n",
            "Saving to: ‘data/clustering_dataset.csv.1’\n",
            "\n",
            "\rclustering_dataset.   0%[                    ]       0  --.-KB/s               \rclustering_dataset. 100%[===================>]     619  --.-KB/s    in 0s      \n",
            "\n",
            "2022-03-28 20:32:31 (25.4 MB/s) - ‘data/clustering_dataset.csv.1’ saved [619/619]\n",
            "\n",
            "--2022-03-28 20:32:31--  https://raw.githubusercontent.com/Renua-Meireles/ML_with_pyspark/main/data/employee.txt\n",
            "Reusing existing connection to raw.githubusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 96027 (94K) [text/plain]\n",
            "Saving to: ‘data/employee.txt.1’\n",
            "\n",
            "\remployee.txt.1        0%[                    ]       0  --.-KB/s               \remployee.txt.1      100%[===================>]  93.78K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-03-28 20:32:31 (6.42 MB/s) - ‘data/employee.txt.1’ saved [96027/96027]\n",
            "\n",
            "--2022-03-28 20:32:31--  https://raw.githubusercontent.com/Renua-Meireles/ML_with_pyspark/main/data/iris.csv\n",
            "Reusing existing connection to raw.githubusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4551 (4.4K) [text/plain]\n",
            "Saving to: ‘data/iris.csv.1’\n",
            "\n",
            "\riris.csv.1            0%[                    ]       0  --.-KB/s               \riris.csv.1          100%[===================>]   4.44K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-03-28 20:32:31 (161 MB/s) - ‘data/iris.csv.1’ saved [4551/4551]\n",
            "\n",
            "--2022-03-28 20:32:31--  https://raw.githubusercontent.com/Renua-Meireles/ML_with_pyspark/main/data/winequality-red.csv\n",
            "Reusing existing connection to raw.githubusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84199 (82K) [text/plain]\n",
            "Saving to: ‘data/winequality-red.csv.1’\n",
            "\n",
            "\rwinequality-red.csv   0%[                    ]       0  --.-KB/s               \rwinequality-red.csv 100%[===================>]  82.23K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2022-03-28 20:32:31 (29.4 MB/s) - ‘data/winequality-red.csv.1’ saved [84199/84199]\n",
            "\n",
            "FINISHED --2022-03-28 20:32:31--\n",
            "Total wall clock time: 0.1s\n",
            "Downloaded: 4 files, 181K in 0.02s (10.4 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzczIr5mt51s"
      },
      "source": [
        "## Installing dependencies\n",
        "> [PySpark](https://pypi.org/project/pyspark/) is now available in pypi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwMFrFKQt51z",
        "outputId": "98eb02ac-96a4-45b6-b188-8a0e91ca16df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4ZAWCw4t512"
      },
      "source": [
        "## Loading PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "Ul5e4qWwt513",
        "outputId": "fa2144ef-cb1c-4665-9c15-a24ee8248a8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7eff784c37d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://274192db0f00:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate() # getting spark session\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nrcyjxy2t514"
      },
      "source": [
        "## Data Preprocessing and Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFv7PSUOt515"
      },
      "source": [
        "### Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLZEKq2Dt517",
        "outputId": "a8e662b3-9671-4d9a-9b73-ae91740345d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+\n",
            "|summary|  id|\n",
            "+-------+----+\n",
            "|  count|1000|\n",
            "+-------+----+\n",
            "\n",
            "+---+------------+--------------------+--------+-------------+------------+------+--------------------+---------+\n",
            "| id|   last_name|               email|  gender|   department|  start_date|salary|           job_title|region_id|\n",
            "+---+------------+--------------------+--------+-------------+------------+------+--------------------+---------+\n",
            "|  1|    'Kelley'|'rkelley0@soundcl...|'Female'|  'Computers'| '10/2/2009'| 67470|'Structural Engin...|        2|\n",
            "|  2| 'Armstrong'|'sarmstrong1@info...|  'Male'|     'Sports'| '3/31/2008'| 71869| 'Financial Advisor'|        2|\n",
            "|  3|      'Carr'|'fcarr2@woothemes...|  'Male'| 'Automotive'| '7/12/2009'|101768|'Recruiting Manager'|        3|\n",
            "|  4|    'Murray'|   'jmurray3@gov.uk'|'Female'|   'Jewelery'|'12/25/2014'| 96897|'Desktop Support ...|        3|\n",
            "|  5|     'Ellis'|'jellis4@scienced...|'Female'|    'Grocery'| '9/19/2002'| 63702|'Software Enginee...|        7|\n",
            "|  6|  'Phillips'|'bphillips5@time....|  'Male'|      'Tools'| '8/21/2013'|118497|'Executive Secret...|        1|\n",
            "|  7|'Williamson'|'rwilliamson6@ted...|  'Male'|  'Computers'| '5/14/2006'| 65889|  'Dental Hygienist'|        6|\n",
            "|  8|    'Harris'| 'aharris7@ucoz.com'|'Female'|       'Toys'| '8/12/2003'| 84427|'Safety Technicia...|        4|\n",
            "|  9|     'James'|'rjames8@prnewswi...|  'Male'|   'Jewelery'|  '9/7/2005'|108657|   'Sales Associate'|        2|\n",
            "| 10|   'Sanchez'|'rsanchez9@cloudf...|  'Male'|     'Movies'| '3/13/2013'|108093|'Sales Representa...|        1|\n",
            "| 11|    'Jacobs'|'jjacobsa@sbwire....|'Female'|   'Jewelery'|'11/27/2003'|121966|'Community Outrea...|        7|\n",
            "| 12|     'Black'|'mblackb@edublogs...|  'Male'|   'Clothing'|  '2/4/2003'| 44179|   'Data Coordiator'|        7|\n",
            "| 13|   'Schmidt'|'sschmidtc@state....|  'Male'|       'Baby'|'10/13/2002'| 85227|'Compensation Ana...|        3|\n",
            "| 14|      'Webb'|  'awebbd@baidu.com'|'Female'|  'Computers'|'10/22/2006'| 59763|'Software Test En...|        4|\n",
            "| 15|    'Jacobs'|'ajacobse@google.it'|'Female'|      'Games'|  '3/4/2007'|141139|'Community Outrea...|        7|\n",
            "| 16|    'Medina'|'smedinaf@amazona...|'Female'|       'Baby'| '3/14/2008'|106659| 'Web Developer III'|        1|\n",
            "| 17|    'Morgan'|'dmorgang@123-reg...|'Female'|       'Kids'|  '5/4/2011'|148952|     'Programmer IV'|        6|\n",
            "| 18|    'Nguyen'|'jnguyenh@google....|  'Male'|       'Home'| '11/3/2014'| 93804|      'Geologist II'|        5|\n",
            "| 19|       'Day'|'rdayi@chronoengi...|  'Male'|'Electronics'| '9/22/2004'|109890|          'VP Sales'|        3|\n",
            "| 20|      'Carr'|  'dcarrj@ocn.ne.jp'|'Female'|     'Movies'|'11/22/2007'|115274|'VP Quality Control'|        5|\n",
            "+---+------------+--------------------+--------+-------------+------------+------+--------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "employees = spark.read.csv('./data/employee.txt', header=True, inferSchema=True)\n",
        "employees.select('id').summary('count').show()\n",
        "employees.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TrAyMySQt519",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc0304bc-88bd-4212-806a-0a5e677da003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- last_name: string (nullable = true)\n",
            " |-- email: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- start_date: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            " |-- job_title: string (nullable = true)\n",
            " |-- region_id: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "employees.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY96jKK4t51-"
      },
      "source": [
        "### Normalize numeric data\n",
        "> Rescale each feature individually to a common range [min, max]. Also known as min-max normalization or Rescaling. <br> See [MinMaxScaler documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.MinMaxScaler.html?highlight=minmaxscaler) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uDpavM9it51_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af251b1b-ff62-43bc-9065-afd0f6f092b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------+--------------------+\n",
            "|salary|salary_vector|       scaled_salary|\n",
            "+------+-------------+--------------------+\n",
            "| 67470|    [67470.0]|[0.24894572414860...|\n",
            "| 71869|    [71869.0]|[0.2890127606087931]|\n",
            "|101768|   [101768.0]| [0.561339271889317]|\n",
            "| 96897|    [96897.0]|[0.5169731580912825]|\n",
            "| 63702|    [63702.0]|[0.21462597116339...|\n",
            "|118497|   [118497.0]|[0.7137105955861592]|\n",
            "| 65889|    [65889.0]|[0.23454563670974...|\n",
            "| 84427|    [84427.0]|[0.40339372079678...|\n",
            "|108657|   [108657.0]|[0.6240857629496043]|\n",
            "|108093|   [108093.0]|[0.6189487298594603]|\n",
            "|121966|   [121966.0]|[0.7453069923764243]|\n",
            "| 44179|    [44179.0]|[0.03680629559799...|\n",
            "| 85227|    [85227.0]|[0.41068029255585...|\n",
            "| 59763|    [59763.0]|[0.1787487134646738]|\n",
            "|141139|   [141139.0]|[0.9199387927972239]|\n",
            "|106659|   [106659.0]|[0.6058875499813282]|\n",
            "|148952|   [148952.0]|[0.9911012742392364]|\n",
            "| 93804|    [93804.0]|[0.4888014500277801]|\n",
            "|109890|   [109890.0]|[0.6353161916732701]|\n",
            "|115274|   [115274.0]|[0.6843548196118079]|\n",
            "+------+-------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import MinMaxScaler\n",
        "\n",
        "# We need to prepare the data as a vector for the transformers to work\n",
        "# We can use the VectorAssembler to do this\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# We will create a pipeline to vectorize the data before we apply the transformers\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "assembler      = VectorAssembler(inputCols=['salary',], outputCol='salary_vector')\n",
        "feature_scaler = MinMaxScaler(inputCol='salary_vector', outputCol='scaled_salary')\n",
        "pipleline      = Pipeline(stages=[assembler, feature_scaler])\n",
        "\n",
        "scaler_model = pipleline.fit(employees)\n",
        "transformed_employees = scaler_model.transform(employees)\n",
        "transformed_employees.select('salary', 'salary_vector', 'scaled_salary').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEXI2X9yt52B"
      },
      "source": [
        "### Standardize numeric data\n",
        "> Standardizes features by removing the mean and scaling to unit variance. <br> See [StandardScaler documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.feature.StandardScaler.html?highlight=standardscaler) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2K5pNuOPt52C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2115068-dfb2-48d4-d796-f2bdbbefd7df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------+--------------------+\n",
            "|salary|salary_vector|        stand_salary|\n",
            "+------+-------------+--------------------+\n",
            "| 67470|    [67470.0]|[-0.9404188679633...|\n",
            "| 71869|    [71869.0]|[-0.8018812534734...|\n",
            "|101768|   [101768.0]|[0.13972732475646...|\n",
            "| 96897|    [96897.0]|[-0.0136749758073...|\n",
            "| 63702|    [63702.0]|[-1.059084412723102]|\n",
            "|118497|   [118497.0]|[0.6665733699489063]|\n",
            "| 65889|    [65889.0]|[-0.9902092677152...|\n",
            "| 84427|    [84427.0]|[-0.4063924235657...|\n",
            "|108657|   [108657.0]|[0.3566824568821564]|\n",
            "|108093|   [108093.0]|[0.33892041674296...|\n",
            "|121966|   [121966.0]|[0.7758225139965237]|\n",
            "| 44179|    [44179.0]|[-1.6739218411582...|\n",
            "| 85227|    [85227.0]|[-0.3811980403895...|\n",
            "| 59763|    [59763.0]|[-1.1831352568867...|\n",
            "|141139|   [141139.0]|[1.3796373997921332]|\n",
            "|106659|   [106659.0]|[0.29375948489970...|\n",
            "|148952|   [148952.0]|[1.6256920444862915]|\n",
            "| 93804|    [93804.0]|[-0.1110827597621...|\n",
            "|109890|   [109890.0]|[0.39551329995241...|\n",
            "|115274|   [115274.0]|[0.5650714987279576]|\n",
            "+------+-------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import StandardScaler\n",
        "\n",
        "# withStd=True and withMean=True sets the standard deviation to 1.0 and the mean to 0.0 respectively\n",
        "feature_stand_scaler = StandardScaler(inputCol='salary_vector', outputCol='stand_salary', withStd=True, withMean=True)\n",
        "pipleline      = Pipeline(stages=[assembler, feature_stand_scaler])\n",
        "\n",
        "scaler_model = pipleline.fit(employees)\n",
        "transformed_employees = scaler_model.transform(employees)\n",
        "transformed_employees.select('salary', 'salary_vector', 'stand_salary').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5JXFk4Kt52D"
      },
      "source": [
        "### Bucketize numeric data\n",
        "> Maps a column of continuous features to a column of feature buckets. <br> See [Bucketizer documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Bucketizer.html) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rg6znVV3t52E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c1033f8-213e-4f9d-da8b-01831bff2349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------+\n",
            "|salary|salary_bucket|\n",
            "+------+-------------+\n",
            "| 67470|          1.0|\n",
            "| 71869|          1.0|\n",
            "|101768|          2.0|\n",
            "| 96897|          1.0|\n",
            "| 63702|          1.0|\n",
            "|118497|          2.0|\n",
            "| 65889|          1.0|\n",
            "| 84427|          1.0|\n",
            "|108657|          2.0|\n",
            "|108093|          2.0|\n",
            "|121966|          2.0|\n",
            "| 44179|          0.0|\n",
            "| 85227|          1.0|\n",
            "| 59763|          1.0|\n",
            "|141139|          3.0|\n",
            "|106659|          2.0|\n",
            "|148952|          3.0|\n",
            "| 93804|          1.0|\n",
            "|109890|          2.0|\n",
            "|115274|          2.0|\n",
            "+------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import Bucketizer\n",
        "\n",
        "bucket_splits = [-float('inf'), 50000.0, 100000.0, 130000.0, float('inf')]\n",
        "\n",
        "# withStd=True and withMean=True sets the standard deviation to 1.0 and the mean to 0.0 respectively\n",
        "bucktzer = Bucketizer(splits=bucket_splits, inputCol='salary', outputCol='salary_bucket')\n",
        "\n",
        "transformed_employees = bucktzer.transform(employees)\n",
        "transformed_employees.select('salary', 'salary_bucket').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdU93LXJt52F"
      },
      "source": [
        "### Tokenize text data\n",
        "> A tokenizer that converts the input string to lowercase and then splits it by white spaces. <br> See [Tokenizer documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Tokenizer.html?highlight=tokenizer) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "q32JM_cxt52F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef7e4ae8-f0b9-425f-a50e-63880e373b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|           job_title|     job_title_words|\n",
            "+--------------------+--------------------+\n",
            "|'Structural Engin...|['structural, eng...|\n",
            "| 'Financial Advisor'|['financial, advi...|\n",
            "|'Recruiting Manager'|['recruiting, man...|\n",
            "|'Desktop Support ...|['desktop, suppor...|\n",
            "|'Software Enginee...|['software, engin...|\n",
            "|'Executive Secret...|['executive, secr...|\n",
            "|  'Dental Hygienist'|['dental, hygieni...|\n",
            "|'Safety Technicia...|['safety, technic...|\n",
            "|   'Sales Associate'|['sales, associate']|\n",
            "|'Sales Representa...|['sales, represen...|\n",
            "|'Community Outrea...|['community, outr...|\n",
            "|   'Data Coordiator'|['data, coordiator']|\n",
            "|'Compensation Ana...|['compensation, a...|\n",
            "|'Software Test En...|['software, test,...|\n",
            "|'Community Outrea...|['community, outr...|\n",
            "| 'Web Developer III'|['web, developer,...|\n",
            "|     'Programmer IV'|  ['programmer, iv']|\n",
            "|      'Geologist II'|   ['geologist, ii']|\n",
            "|          'VP Sales'|       ['vp, sales']|\n",
            "|'VP Quality Control'|['vp, quality, co...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(inputCol='job_title', outputCol='job_title_words')\n",
        "\n",
        "transformed_employees = tokenizer.transform(employees)\n",
        "transformed_employees.select('job_title', 'job_title_words').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA2R5hxNt52G"
      },
      "source": [
        "### Get TF-IDF\n",
        "> Maps a sequence of terms to their term frequencies using the hashing trick. <br> See [HashingTF documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.feature.HashingTF.html?highlight=hashingtf) for more details.\n",
        "\n",
        "> Inverse document frequency. <br> See [IDF documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.feature.IDF.html?highlight=idf) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "V-fNL6APt52H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e3c8023-78cc-4ee2-f470-9a69145b80dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(job_title=\"'Structural Engineer'\", job_title_words=[\"'structural\", \"engineer'\"], job_title_tf=SparseVector(20, {11: 1.0, 17: 1.0}), job_title_tfidf=SparseVector(20, {11: 1.4034, 17: 2.552}))]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "\n",
        "hashingTF = HashingTF(inputCol='job_title_words', outputCol='job_title_tf', numFeatures=20)\n",
        "idf       = IDF(inputCol='job_title_tf', outputCol='job_title_tfidf')\n",
        "\n",
        "# putting all the transformers together in a pipeline\n",
        "tokenizer = Tokenizer(inputCol='job_title', outputCol='job_title_words')\n",
        "pipleline = Pipeline(stages=[tokenizer, hashingTF, idf])\n",
        "\n",
        "transformed_employees = pipleline.fit(employees).transform(employees)\n",
        "transformed_employees.select('job_title', 'job_title_words', 'job_title_tf', 'job_title_tfidf').take(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzHNk7Gpt52J"
      },
      "source": [
        "## Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rid9ie9ft52J"
      },
      "source": [
        "### Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zXE9nptzt52L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1b910d9-0b59-4632-88ab-faa922740b2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------+----+----+\n",
            "|summary|             col1|col2|col3|\n",
            "+-------+-----------------+----+----+\n",
            "|    min|                1|   1|   1|\n",
            "|   mean|39.74666666666667|38.2|38.6|\n",
            "|    max|               99| 100| 100|\n",
            "|  count|               75|  75|  75|\n",
            "+-------+-----------------+----+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = spark.read.csv('./data/clustering_dataset.csv', header=True, inferSchema=True)\n",
        "dataset.summary('min','mean', 'max','count').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IPzvEE-t52L"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Pkn9QewZt52M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8031b52f-fbc1-4a21-f0e7-c77680bb543f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+--------------+\n",
            "|col1|col2|col3|      features|\n",
            "+----+----+----+--------------+\n",
            "|   7|   4|   1| [7.0,4.0,1.0]|\n",
            "|   7|   7|   9| [7.0,7.0,9.0]|\n",
            "|   7|   9|   6| [7.0,9.0,6.0]|\n",
            "|   1|   6|   5| [1.0,6.0,5.0]|\n",
            "|   6|   7|   7| [6.0,7.0,7.0]|\n",
            "|   7|   9|   4| [7.0,9.0,4.0]|\n",
            "|   7|  10|   6|[7.0,10.0,6.0]|\n",
            "|   7|   8|   2| [7.0,8.0,2.0]|\n",
            "|   8|   3|   8| [8.0,3.0,8.0]|\n",
            "|   4|  10|   5|[4.0,10.0,5.0]|\n",
            "|   7|   4|   5| [7.0,4.0,5.0]|\n",
            "|   7|   8|   4| [7.0,8.0,4.0]|\n",
            "|   2|   5|   1| [2.0,5.0,1.0]|\n",
            "|   2|   6|   2| [2.0,6.0,2.0]|\n",
            "|   2|   3|   8| [2.0,3.0,8.0]|\n",
            "|   3|   9|   1| [3.0,9.0,1.0]|\n",
            "|   4|   2|   9| [4.0,2.0,9.0]|\n",
            "|   1|   7|   1| [1.0,7.0,1.0]|\n",
            "|   6|   2|   3| [6.0,2.0,3.0]|\n",
            "|   4|   1|   9| [4.0,1.0,9.0]|\n",
            "+----+----+----+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# just joining the columns to feature vector\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(inputCols=['col1', 'col2', 'col3'], outputCol='features')\n",
        "dataset = assembler.transform(dataset)\n",
        "dataset.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyxjnXYVt52N"
      },
      "source": [
        "### K-Means clustering\n",
        "\n",
        "> See [KMeans documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.KMeans.html?highlight=kmeans#pyspark.ml.clustering.KMeans) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DLRdwMtit52N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30dca8ee-68af-4af4-9443-6b731fd91eb8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([35.88461538, 31.46153846, 34.42307692]),\n",
              " array([80.        , 79.20833333, 78.29166667]),\n",
              " array([5.12, 5.84, 4.84])]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# 3 clusters\n",
        "kmeans = KMeans(featuresCol='features', k=3, seed=1)\n",
        "model = kmeans.fit(dataset)\n",
        "model.clusterCenters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGVl4Lfyt52O"
      },
      "source": [
        "### Hierarchical clustering - BisectingKMeans\n",
        "\n",
        "> See [BisectingKMeans documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.BisectingKMeans.html?highlight=bisectingkmeans#pyspark.ml.clustering.BisectingKMeans) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PsU0P5UNt52O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71893b30-1eb5-4b66-b803-f509dd313547"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([5.12, 5.84, 4.84]),\n",
              " array([35.88461538, 31.46153846, 34.42307692]),\n",
              " array([80.        , 79.20833333, 78.29166667])]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from pyspark.ml.clustering import BisectingKMeans\n",
        "\n",
        "# 3 clusters\n",
        "kmeans = BisectingKMeans(featuresCol='features', k=3, seed=1)\n",
        "model = kmeans.fit(dataset)\n",
        "model.clusterCenters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEpkeXUst52P"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkGwreSGt52P"
      },
      "source": [
        "### Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fbuPwKL2t52Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb31ec5b-668d-4888-c0d1-9ce8548aebf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+---+-----------+\n",
            "|_c0|_c1|_c2|_c3|        _c4|\n",
            "+---+---+---+---+-----------+\n",
            "|5.1|3.5|1.4|0.2|Iris-setosa|\n",
            "|4.9|3.0|1.4|0.2|Iris-setosa|\n",
            "|4.7|3.2|1.3|0.2|Iris-setosa|\n",
            "|4.6|3.1|1.5|0.2|Iris-setosa|\n",
            "|5.0|3.6|1.4|0.2|Iris-setosa|\n",
            "|5.4|3.9|1.7|0.4|Iris-setosa|\n",
            "|4.6|3.4|1.4|0.3|Iris-setosa|\n",
            "|5.0|3.4|1.5|0.2|Iris-setosa|\n",
            "|4.4|2.9|1.4|0.2|Iris-setosa|\n",
            "|4.9|3.1|1.5|0.1|Iris-setosa|\n",
            "|5.4|3.7|1.5|0.2|Iris-setosa|\n",
            "|4.8|3.4|1.6|0.2|Iris-setosa|\n",
            "|4.8|3.0|1.4|0.1|Iris-setosa|\n",
            "|4.3|3.0|1.1|0.1|Iris-setosa|\n",
            "|5.8|4.0|1.2|0.2|Iris-setosa|\n",
            "|5.7|4.4|1.5|0.4|Iris-setosa|\n",
            "|5.4|3.9|1.3|0.4|Iris-setosa|\n",
            "|5.1|3.5|1.4|0.3|Iris-setosa|\n",
            "|5.7|3.8|1.7|0.3|Iris-setosa|\n",
            "|5.1|3.8|1.5|0.3|Iris-setosa|\n",
            "+---+---+---+---+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = spark.read.csv('./data/iris.csv', inferSchema=True)\n",
        "dataset.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAo2Q6bst52Q"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "> **Iris** [Dataset Link](https://archive.ics.uci.edu/ml/datasets/iris).\n",
        "\n",
        "- Creating a pipeline of transformations [RenameColumns -> JoinFeatureColumns -> EncodeTargetLabels -> splitTrainTest]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3zXemIyUt52Q"
      },
      "outputs": [],
      "source": [
        "# Creating a custom transformer to rename the columns to friendly names\n",
        "from pyspark.ml import Transformer\n",
        "\n",
        "class ColumnRenamer(Transformer):\n",
        "    def __init__(self, ):\n",
        "        super().__init__()\n",
        "\n",
        "    def _transform(self, df):\n",
        "        return df.selectExpr(\n",
        "                            '_c0 AS sepal_length',\n",
        "                            '_c1 AS sepal_width',\n",
        "                            '_c2 AS petal_length',\n",
        "                            '_c3 AS petal_width',\n",
        "                            '_c4 AS species')\n",
        "column_renamer = ColumnRenamer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dBpW48LEt52R"
      },
      "outputs": [],
      "source": [
        "# Creating a transformer to join columns to feature vector\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(inputCols=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], outputCol='features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Wpc6U1rut52S"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "string_indexer = StringIndexer(inputCol='species', outputCol='class')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Y1Jh7bD_t52S"
      },
      "outputs": [],
      "source": [
        "# Creating a custom transformer to split the dataframe into training and test data\n",
        "\n",
        "class TrainTestDataSplitter(Transformer):\n",
        "    def __init__(self, ):\n",
        "        super().__init__()\n",
        "\n",
        "    def _transform(self, df):\n",
        "        return df.randomSplit([0.7, 0.3], 1)\n",
        "train_test_data_splitter = TrainTestDataSplitter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Oq_UICkLt52S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "2ccc56f0-d882-46d8-b547-d179f7285d59"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "110"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[Row(sepal_length=4.3, sepal_width=3.0, petal_length=1.1, petal_width=0.1, species='Iris-setosa', features=DenseVector([4.3, 3.0, 1.1, 0.1]), class=0.0)]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Assembling the pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "pipeline = Pipeline(stages=[column_renamer, assembler, string_indexer, train_test_data_splitter])\n",
        "train_df, test_df = pipeline.fit(dataset).transform(dataset)\n",
        "display(\n",
        "    dataset.count(),\n",
        "    train_df.count(),\n",
        "    test_df.count(),\n",
        "    train_df.take(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfJJu2kYt52U"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ByQDx9bOt52U"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "def train_and_evaluate_classification_model(classifier, df_train, df_test) -> float:\n",
        "    model = classifier.fit(df_train)\n",
        "\n",
        "    predictions = model.transform(df_test)\n",
        "\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol='class', predictionCol='prediction', metricName='accuracy')\n",
        "    acc = evaluator.evaluate(predictions)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLrUtouIt52V"
      },
      "source": [
        "#### Naive Bayes\n",
        "\n",
        "> Naive Bayes Classifiers. See [NaiveBayes documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.NaiveBayes.html?highlight=naivebayes#pyspark.ml.classification.NaiveBayes) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wj2KcxLXt52W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11f5cf97-975c-4856-9740-9b88599747d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.75"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "\n",
        "naive = NaiveBayes(featuresCol='features', labelCol='class', predictionCol='prediction')\n",
        "train_and_evaluate_classification_model(naive, train_df, test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nYn5MRKt52W"
      },
      "source": [
        "#### Multilayer Perceptron Classifier\n",
        "\n",
        "> Classifier trainer based on the Multilayer Perceptron. Each layer has sigmoid activation function, output layer has softmax. See [MultilayerPerceptronClassifier documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.MultilayerPerceptronClassifier.html?highlight=multilayer%20perceptron%20classifier#pyspark.ml.classification.MultilayerPerceptronClassifier) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "f60NP7art52X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38d08e99-d6bd-4b40-f759-fe1f5f572e3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.975"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "\n",
        "# 4 hidden layers: 4 (features), 4, 4, 3 (classes)\n",
        "mlp = MultilayerPerceptronClassifier(layers=[4, 4, 4, 3], featuresCol='features', labelCol='class', predictionCol='prediction')\n",
        "train_and_evaluate_classification_model(mlp, train_df, test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKItNf56t52X"
      },
      "source": [
        "#### Decision Tree Classifier\n",
        "\n",
        "> Decision tree learning algorithm for classification. See [DecisionTreeClassifier documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.DecisionTreeClassifier.html?highlight=decision%20trees%20classifiers) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "er3m8sF6t52Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b07588-3ab2-4b70-a2bc-045a5ddabb19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.925"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "tree = DecisionTreeClassifier(featuresCol='features', labelCol='class', predictionCol='prediction')\n",
        "train_and_evaluate_classification_model(tree, train_df, test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btZFSlEmt52Y"
      },
      "source": [
        "## Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qflQNx6Bt52Z"
      },
      "source": [
        "### Loading dataset\n",
        "\n",
        "> **Red-Wine Quality** [Dataset Link](https://archive.ics.uci.edu/ml/datasets/wine+quality)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "egVp_LVGt52Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "130ef040-d4a0-4242-f7e7-59fc37a142e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- fixed acidity: double (nullable = true)\n",
            " |-- volatile acidity: double (nullable = true)\n",
            " |-- citric acid: double (nullable = true)\n",
            " |-- residual sugar: double (nullable = true)\n",
            " |-- chlorides: double (nullable = true)\n",
            " |-- free sulfur dioxide: double (nullable = true)\n",
            " |-- total sulfur dioxide: double (nullable = true)\n",
            " |-- density: double (nullable = true)\n",
            " |-- pH: double (nullable = true)\n",
            " |-- sulphates: double (nullable = true)\n",
            " |-- alcohol: double (nullable = true)\n",
            " |-- quality: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.csv('./data/winequality-red.csv', header=True, inferSchema=True, sep=';')\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7NTo3Hpt52Z"
      },
      "source": [
        "### Exploring dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S8dtnLut52a"
      },
      "source": [
        "#### Looking for columns with null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MoZRfyAot52b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22bee33e-2ffa-49ca-e5e5-a12180797955"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "columns_with_null_values = [column for column in df.columns if df.filter(f\"'{column}' is NULL\").count() > 0]\n",
        "columns_with_null_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ehb1cjdt52c"
      },
      "source": [
        "#### Analysing columns statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xhyyG75Tt52d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae19a56b-e34b-4b44-99ad-8ac74bd51cdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+\n",
            "|summary|fixed acidity|\n",
            "+-------+-------------+\n",
            "|  count|       1599.0|\n",
            "|   mean|         8.32|\n",
            "| stddev|         1.74|\n",
            "|    min|          4.6|\n",
            "|    max|         15.9|\n",
            "+-------+-------------+\n",
            "\n",
            "+-------+----------------+\n",
            "|summary|volatile acidity|\n",
            "+-------+----------------+\n",
            "|  count|          1599.0|\n",
            "|   mean|            0.53|\n",
            "| stddev|            0.18|\n",
            "|    min|            0.12|\n",
            "|    max|            1.58|\n",
            "+-------+----------------+\n",
            "\n",
            "+-------+-----------+\n",
            "|summary|citric acid|\n",
            "+-------+-----------+\n",
            "|  count|     1599.0|\n",
            "|   mean|       0.27|\n",
            "| stddev|       0.19|\n",
            "|    min|        0.0|\n",
            "|    max|        1.0|\n",
            "+-------+-----------+\n",
            "\n",
            "+-------+--------------+\n",
            "|summary|residual sugar|\n",
            "+-------+--------------+\n",
            "|  count|        1599.0|\n",
            "|   mean|          2.54|\n",
            "| stddev|          1.41|\n",
            "|    min|           0.9|\n",
            "|    max|          15.5|\n",
            "+-------+--------------+\n",
            "\n",
            "+-------+---------+\n",
            "|summary|chlorides|\n",
            "+-------+---------+\n",
            "|  count|   1599.0|\n",
            "|   mean|     0.09|\n",
            "| stddev|     0.05|\n",
            "|    min|     0.01|\n",
            "|    max|     0.61|\n",
            "+-------+---------+\n",
            "\n",
            "+-------+-------------------+\n",
            "|summary|free sulfur dioxide|\n",
            "+-------+-------------------+\n",
            "|  count|             1599.0|\n",
            "|   mean|              15.87|\n",
            "| stddev|              10.46|\n",
            "|    min|                1.0|\n",
            "|    max|               72.0|\n",
            "+-------+-------------------+\n",
            "\n",
            "+-------+--------------------+\n",
            "|summary|total sulfur dioxide|\n",
            "+-------+--------------------+\n",
            "|  count|              1599.0|\n",
            "|   mean|               46.47|\n",
            "| stddev|                32.9|\n",
            "|    min|                 6.0|\n",
            "|    max|               289.0|\n",
            "+-------+--------------------+\n",
            "\n",
            "+-------+-------+\n",
            "|summary|density|\n",
            "+-------+-------+\n",
            "|  count| 1599.0|\n",
            "|   mean|    1.0|\n",
            "| stddev|    0.0|\n",
            "|    min|   0.99|\n",
            "|    max|    1.0|\n",
            "+-------+-------+\n",
            "\n",
            "+-------+------+\n",
            "|summary|    pH|\n",
            "+-------+------+\n",
            "|  count|1599.0|\n",
            "|   mean|  3.31|\n",
            "| stddev|  0.15|\n",
            "|    min|  2.74|\n",
            "|    max|  4.01|\n",
            "+-------+------+\n",
            "\n",
            "+-------+---------+\n",
            "|summary|sulphates|\n",
            "+-------+---------+\n",
            "|  count|   1599.0|\n",
            "|   mean|     0.66|\n",
            "| stddev|     0.17|\n",
            "|    min|     0.33|\n",
            "|    max|      2.0|\n",
            "+-------+---------+\n",
            "\n",
            "+-------+-------+\n",
            "|summary|alcohol|\n",
            "+-------+-------+\n",
            "|  count| 1599.0|\n",
            "|   mean|  10.42|\n",
            "| stddev|   1.07|\n",
            "|    min|    8.4|\n",
            "|    max|   14.9|\n",
            "+-------+-------+\n",
            "\n",
            "+-------+-------+\n",
            "|summary|quality|\n",
            "+-------+-------+\n",
            "|  count| 1599.0|\n",
            "|   mean|   5.64|\n",
            "| stddev|   0.81|\n",
            "|    min|    3.0|\n",
            "|    max|    8.0|\n",
            "+-------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import round\n",
        "for col in df.columns:\n",
        "    # round column values to 2 decimal places\n",
        "    df.select(col).describe().select('summary', round(col, 2).alias(col)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxYimUc4t52f"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FzTI8mCxt52g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "364ff6e3-d29f-42a9-a006-1e3dac91d7ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------+\n",
            "|            features|quality|\n",
            "+--------------------+-------+\n",
            "|[7.4,0.7,0.0,1.9,...|      5|\n",
            "|[7.8,0.88,0.0,2.6...|      5|\n",
            "|[7.8,0.76,0.04,2....|      5|\n",
            "|[11.2,0.28,0.56,1...|      6|\n",
            "|[7.4,0.7,0.0,1.9,...|      5|\n",
            "|[7.4,0.66,0.0,1.8...|      5|\n",
            "|[7.9,0.6,0.06,1.6...|      5|\n",
            "|[7.3,0.65,0.0,1.2...|      7|\n",
            "|[7.8,0.58,0.02,2....|      7|\n",
            "|[7.5,0.5,0.36,6.1...|      5|\n",
            "|[6.7,0.58,0.08,1....|      5|\n",
            "|[7.5,0.5,0.36,6.1...|      5|\n",
            "|[5.6,0.615,0.0,1....|      5|\n",
            "|[7.8,0.61,0.29,1....|      5|\n",
            "|[8.9,0.62,0.18,3....|      5|\n",
            "|[8.9,0.62,0.19,3....|      5|\n",
            "|[8.5,0.28,0.56,1....|      7|\n",
            "|[8.1,0.56,0.28,1....|      5|\n",
            "|[7.4,0.59,0.08,4....|      4|\n",
            "|[7.9,0.32,0.51,1....|      6|\n",
            "+--------------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Joining feature columns\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "target_column = 'quality'\n",
        "features = df.columns\n",
        "features.remove(target_column)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol='features')\n",
        "df = assembler.transform(df).select('features', target_column)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "JPxgnV05t52h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "790b251f-fa56-440b-a15b-745218b454ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------+\n",
            "|            features|quality|\n",
            "+--------------------+-------+\n",
            "|[-0.5281943702487...|      5|\n",
            "|[-0.2984540647668...|      5|\n",
            "|[-0.2984540647668...|      5|\n",
            "|[1.65433853182897...|      6|\n",
            "|[-0.5281943702487...|      5|\n",
            "|[-0.5281943702487...|      5|\n",
            "|[-0.2410189883963...|      5|\n",
            "|[-0.5856294466191...|      7|\n",
            "|[-0.2984540647668...|      7|\n",
            "|[-0.4707592938782...|      5|\n",
            "|[-0.9302399048419...|      5|\n",
            "|[-0.4707592938782...|      5|\n",
            "|[-1.5620257449170...|      5|\n",
            "|[-0.2984540647668...|      5|\n",
            "|[0.33333177530827...|      5|\n",
            "|[0.33333177530827...|      5|\n",
            "|[0.10359146982641...|      7|\n",
            "|[-0.1261488356554...|      5|\n",
            "|[-0.5281943702487...|      4|\n",
            "|[-0.2410189883963...|      6|\n",
            "+--------------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Standardizing the data\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.sql.functions import column\n",
        "\n",
        "standardizer = StandardScaler(inputCol='features', outputCol='scaled_features', withStd=True, withMean=True)\n",
        "df = standardizer.fit(df).transform(df).select(column('scaled_features').alias('features'), target_column)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "UzIUPU40t52i"
      },
      "outputs": [],
      "source": [
        "# Splitting the data into training and test data\n",
        "\n",
        "df_train, df_test = df.randomSplit([0.7, 0.3], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhc590Sft52j"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "dEInuLMvt52k"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import round\n",
        "\n",
        "def train_and_evaluate_regression_model(regressor, df_train, df_test):\n",
        "    model = regressor.fit(df_train)\n",
        "\n",
        "    predictions = model.transform(df_test)\n",
        "\n",
        "    # Evaluate the model aproximation to real data\n",
        "    evaluator = RegressionEvaluator(labelCol='quality', predictionCol='prediction', metricName='rmse')\n",
        "    rmse = evaluator.evaluate(predictions)\n",
        "\n",
        "    # Evaluating the integer value prediction\n",
        "    class_evaluator = MulticlassClassificationEvaluator(labelCol='quality', predictionCol='prediction', metricName='accuracy')\n",
        "    acc = class_evaluator.evaluate(predictions.select(round('prediction', 0).alias('prediction'), 'quality'))\n",
        "    return rmse, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPhEP9Ost52l"
      },
      "source": [
        "#### Linear Regression\n",
        "\n",
        "> The learning objective is to minimize the specified loss function, with regularization. See [LinearRegression documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegression.html?highlight=linear%20regression#pyspark.ml.regression.LinearRegression) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "sgdJy8EPt52m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ba0c993-347c-4fc4-cdbd-e8490783c08a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6434870482395909, 0.558695652173913)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "lr = LinearRegression(featuresCol='features', labelCol='quality', predictionCol='prediction')\n",
        "train_and_evaluate_regression_model(lr, df_train, df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyk-2W35t52n"
      },
      "source": [
        "#### Decision Tree Regressor\n",
        "\n",
        "> Decision tree learning algorithm for regression. See [DecisionTreeRegressor documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.DecisionTreeRegressor.html#pyspark.ml.regression.DecisionTreeRegressor) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "2w_5Zf6Bt52o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5de0c06b-edd1-434c-ce8e-32feb4d6f6ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6660182397865323, 0.591304347826087)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "\n",
        "tree = DecisionTreeRegressor(featuresCol='features', labelCol='quality', predictionCol='prediction')\n",
        "train_and_evaluate_regression_model(tree, df_train, df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_3AhqNxt52p"
      },
      "source": [
        "#### Gradient-Boosted Trees (GBTs)\n",
        "\n",
        "> Gradient-Boosted Trees (GBTs) learning algorithm. See [GBTRegressor documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.GBTRegressor.html?highlight=gbt%20regression) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "2cifClg1t52q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ccb7078-385c-4d17-c750-7a421d90bc9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6535988383824605, 0.5978260869565217)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "from pyspark.ml.regression import GBTRegressor\n",
        "\n",
        "gbt = GBTRegressor(featuresCol='features', labelCol='quality', predictionCol='prediction')\n",
        "train_and_evaluate_regression_model(gbt, df_train, df_test)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "91e1978be154cf5421f2f21f3417c39a219e9c14846f040d23c9fcd9f86222c3"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "ml_with_pyspark.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}