{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies\n",
    "> [PySpark](https://pypi.org/project/pyspark/) is now available in pypi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in d:\\documents\\programming\\pyspark\\coding\\ml_with_pyspark\\venv\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in d:\\documents\\programming\\pyspark\\coding\\ml_with_pyspark\\venv\\lib\\site-packages (from pyspark) (0.10.9.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'd:\\documents\\programming\\pyspark\\coding\\ml_with_pyspark\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21e9c971460>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() # getting spark session\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|summary|  id|\n",
      "+-------+----+\n",
      "|  count|1000|\n",
      "+-------+----+\n",
      "\n",
      "+---+------------+--------------------+--------+-------------+------------+------+--------------------+---------+\n",
      "| id|   last_name|               email|  gender|   department|  start_date|salary|           job_title|region_id|\n",
      "+---+------------+--------------------+--------+-------------+------------+------+--------------------+---------+\n",
      "|  1|    'Kelley'|'rkelley0@soundcl...|'Female'|  'Computers'| '10/2/2009'| 67470|'Structural Engin...|        2|\n",
      "|  2| 'Armstrong'|'sarmstrong1@info...|  'Male'|     'Sports'| '3/31/2008'| 71869| 'Financial Advisor'|        2|\n",
      "|  3|      'Carr'|'fcarr2@woothemes...|  'Male'| 'Automotive'| '7/12/2009'|101768|'Recruiting Manager'|        3|\n",
      "|  4|    'Murray'|   'jmurray3@gov.uk'|'Female'|   'Jewelery'|'12/25/2014'| 96897|'Desktop Support ...|        3|\n",
      "|  5|     'Ellis'|'jellis4@scienced...|'Female'|    'Grocery'| '9/19/2002'| 63702|'Software Enginee...|        7|\n",
      "|  6|  'Phillips'|'bphillips5@time....|  'Male'|      'Tools'| '8/21/2013'|118497|'Executive Secret...|        1|\n",
      "|  7|'Williamson'|'rwilliamson6@ted...|  'Male'|  'Computers'| '5/14/2006'| 65889|  'Dental Hygienist'|        6|\n",
      "|  8|    'Harris'| 'aharris7@ucoz.com'|'Female'|       'Toys'| '8/12/2003'| 84427|'Safety Technicia...|        4|\n",
      "|  9|     'James'|'rjames8@prnewswi...|  'Male'|   'Jewelery'|  '9/7/2005'|108657|   'Sales Associate'|        2|\n",
      "| 10|   'Sanchez'|'rsanchez9@cloudf...|  'Male'|     'Movies'| '3/13/2013'|108093|'Sales Representa...|        1|\n",
      "| 11|    'Jacobs'|'jjacobsa@sbwire....|'Female'|   'Jewelery'|'11/27/2003'|121966|'Community Outrea...|        7|\n",
      "| 12|     'Black'|'mblackb@edublogs...|  'Male'|   'Clothing'|  '2/4/2003'| 44179|   'Data Coordiator'|        7|\n",
      "| 13|   'Schmidt'|'sschmidtc@state....|  'Male'|       'Baby'|'10/13/2002'| 85227|'Compensation Ana...|        3|\n",
      "| 14|      'Webb'|  'awebbd@baidu.com'|'Female'|  'Computers'|'10/22/2006'| 59763|'Software Test En...|        4|\n",
      "| 15|    'Jacobs'|'ajacobse@google.it'|'Female'|      'Games'|  '3/4/2007'|141139|'Community Outrea...|        7|\n",
      "| 16|    'Medina'|'smedinaf@amazona...|'Female'|       'Baby'| '3/14/2008'|106659| 'Web Developer III'|        1|\n",
      "| 17|    'Morgan'|'dmorgang@123-reg...|'Female'|       'Kids'|  '5/4/2011'|148952|     'Programmer IV'|        6|\n",
      "| 18|    'Nguyen'|'jnguyenh@google....|  'Male'|       'Home'| '11/3/2014'| 93804|      'Geologist II'|        5|\n",
      "| 19|       'Day'|'rdayi@chronoengi...|  'Male'|'Electronics'| '9/22/2004'|109890|          'VP Sales'|        3|\n",
      "| 20|      'Carr'|  'dcarrj@ocn.ne.jp'|'Female'|     'Movies'|'11/22/2007'|115274|'VP Quality Control'|        5|\n",
      "+---+------------+--------------------+--------+-------------+------------+------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees = spark.read.csv('./data/employee.txt', header=True, inferSchema=True)\n",
    "employees.select('id').summary('count').show()\n",
    "employees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- region_id: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employees.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize numeric data\n",
    "> Rescale each feature individually to a common range [min, max]. Also known as min-max normalization or Rescaling. <br> See [MinMaxScaler documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.MinMaxScaler.html?highlight=minmaxscaler) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------------------+\n",
      "|salary|salary_vector|       scaled_salary|\n",
      "+------+-------------+--------------------+\n",
      "| 67470|    [67470.0]|[0.24894572414860...|\n",
      "| 71869|    [71869.0]|[0.2890127606087931]|\n",
      "|101768|   [101768.0]| [0.561339271889317]|\n",
      "| 96897|    [96897.0]|[0.5169731580912825]|\n",
      "| 63702|    [63702.0]|[0.21462597116339...|\n",
      "|118497|   [118497.0]|[0.7137105955861592]|\n",
      "| 65889|    [65889.0]|[0.23454563670974...|\n",
      "| 84427|    [84427.0]|[0.40339372079678...|\n",
      "|108657|   [108657.0]|[0.6240857629496043]|\n",
      "|108093|   [108093.0]|[0.6189487298594603]|\n",
      "|121966|   [121966.0]|[0.7453069923764243]|\n",
      "| 44179|    [44179.0]|[0.03680629559799...|\n",
      "| 85227|    [85227.0]|[0.41068029255585...|\n",
      "| 59763|    [59763.0]|[0.1787487134646738]|\n",
      "|141139|   [141139.0]|[0.9199387927972239]|\n",
      "|106659|   [106659.0]|[0.6058875499813282]|\n",
      "|148952|   [148952.0]|[0.9911012742392364]|\n",
      "| 93804|    [93804.0]|[0.4888014500277801]|\n",
      "|109890|   [109890.0]|[0.6353161916732701]|\n",
      "|115274|   [115274.0]|[0.6843548196118079]|\n",
      "+------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "# We need to prepare the data as a vector for the transformers to work\n",
    "# We can use the VectorAssembler to do this\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# We will create a pipeline to vectorize the data before we apply the transformers\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "assembler      = VectorAssembler(inputCols=['salary',], outputCol='salary_vector')\n",
    "feature_scaler = MinMaxScaler(inputCol='salary_vector', outputCol='scaled_salary')\n",
    "pipleline      = Pipeline(stages=[assembler, feature_scaler])\n",
    "\n",
    "scaler_model = pipleline.fit(employees)\n",
    "transformed_employees = scaler_model.transform(employees)\n",
    "transformed_employees.select('salary', 'salary_vector', 'scaled_salary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize numeric data\n",
    "> Standardizes features by removing the mean and scaling to unit variance. <br> See [StandardScaler documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.feature.StandardScaler.html?highlight=standardscaler) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------------------+\n",
      "|salary|salary_vector|        stand_salary|\n",
      "+------+-------------+--------------------+\n",
      "| 67470|    [67470.0]|[-0.9404188679633...|\n",
      "| 71869|    [71869.0]|[-0.8018812534734...|\n",
      "|101768|   [101768.0]|[0.13972732475646...|\n",
      "| 96897|    [96897.0]|[-0.0136749758073...|\n",
      "| 63702|    [63702.0]|[-1.059084412723102]|\n",
      "|118497|   [118497.0]|[0.6665733699489063]|\n",
      "| 65889|    [65889.0]|[-0.9902092677152...|\n",
      "| 84427|    [84427.0]|[-0.4063924235657...|\n",
      "|108657|   [108657.0]|[0.3566824568821564]|\n",
      "|108093|   [108093.0]|[0.33892041674296...|\n",
      "|121966|   [121966.0]|[0.7758225139965237]|\n",
      "| 44179|    [44179.0]|[-1.6739218411582...|\n",
      "| 85227|    [85227.0]|[-0.3811980403895...|\n",
      "| 59763|    [59763.0]|[-1.1831352568867...|\n",
      "|141139|   [141139.0]|[1.3796373997921332]|\n",
      "|106659|   [106659.0]|[0.29375948489970...|\n",
      "|148952|   [148952.0]|[1.6256920444862915]|\n",
      "| 93804|    [93804.0]|[-0.1110827597621...|\n",
      "|109890|   [109890.0]|[0.39551329995241...|\n",
      "|115274|   [115274.0]|[0.5650714987279576]|\n",
      "+------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# withStd=True and withMean=True sets the standard deviation to 1.0 and the mean to 0.0 respectively\n",
    "feature_stand_scaler = StandardScaler(inputCol='salary_vector', outputCol='stand_salary', withStd=True, withMean=True)\n",
    "pipleline      = Pipeline(stages=[assembler, feature_stand_scaler])\n",
    "\n",
    "scaler_model = pipleline.fit(employees)\n",
    "transformed_employees = scaler_model.transform(employees)\n",
    "transformed_employees.select('salary', 'salary_vector', 'stand_salary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketize numeric data\n",
    "> Maps a column of continuous features to a column of feature buckets. <br> See [Bucketizer documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Bucketizer.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|salary|salary_bucket|\n",
      "+------+-------------+\n",
      "| 67470|          1.0|\n",
      "| 71869|          1.0|\n",
      "|101768|          2.0|\n",
      "| 96897|          1.0|\n",
      "| 63702|          1.0|\n",
      "|118497|          2.0|\n",
      "| 65889|          1.0|\n",
      "| 84427|          1.0|\n",
      "|108657|          2.0|\n",
      "|108093|          2.0|\n",
      "|121966|          2.0|\n",
      "| 44179|          0.0|\n",
      "| 85227|          1.0|\n",
      "| 59763|          1.0|\n",
      "|141139|          3.0|\n",
      "|106659|          2.0|\n",
      "|148952|          3.0|\n",
      "| 93804|          1.0|\n",
      "|109890|          2.0|\n",
      "|115274|          2.0|\n",
      "+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "bucket_splits = [-float('inf'), 50000.0, 100000.0, 130000.0, float('inf')]\n",
    "\n",
    "# withStd=True and withMean=True sets the standard deviation to 1.0 and the mean to 0.0 respectively\n",
    "bucktzer = Bucketizer(splits=bucket_splits, inputCol='salary', outputCol='salary_bucket')\n",
    "\n",
    "transformed_employees = bucktzer.transform(employees)\n",
    "transformed_employees.select('salary', 'salary_bucket').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text data\n",
    "> A tokenizer that converts the input string to lowercase and then splits it by white spaces. <br> See [Tokenizer documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Tokenizer.html?highlight=tokenizer) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|           job_title|     job_title_words|\n",
      "+--------------------+--------------------+\n",
      "|'Structural Engin...|['structural, eng...|\n",
      "| 'Financial Advisor'|['financial, advi...|\n",
      "|'Recruiting Manager'|['recruiting, man...|\n",
      "|'Desktop Support ...|['desktop, suppor...|\n",
      "|'Software Enginee...|['software, engin...|\n",
      "|'Executive Secret...|['executive, secr...|\n",
      "|  'Dental Hygienist'|['dental, hygieni...|\n",
      "|'Safety Technicia...|['safety, technic...|\n",
      "|   'Sales Associate'|['sales, associate']|\n",
      "|'Sales Representa...|['sales, represen...|\n",
      "|'Community Outrea...|['community, outr...|\n",
      "|   'Data Coordiator'|['data, coordiator']|\n",
      "|'Compensation Ana...|['compensation, a...|\n",
      "|'Software Test En...|['software, test,...|\n",
      "|'Community Outrea...|['community, outr...|\n",
      "| 'Web Developer III'|['web, developer,...|\n",
      "|     'Programmer IV'|  ['programmer, iv']|\n",
      "|      'Geologist II'|   ['geologist, ii']|\n",
      "|          'VP Sales'|       ['vp, sales']|\n",
      "|'VP Quality Control'|['vp, quality, co...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='job_title', outputCol='job_title_words')\n",
    "\n",
    "transformed_employees = tokenizer.transform(employees)\n",
    "transformed_employees.select('job_title', 'job_title_words').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get TF-IDF\n",
    "> Maps a sequence of terms to their term frequencies using the hashing trick. <br> See [HashingTF documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.feature.HashingTF.html?highlight=hashingtf) for more details.\n",
    "\n",
    "> Inverse document frequency. <br> See [IDF documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.feature.IDF.html?highlight=idf) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(job_title=\"'Structural Engineer'\", job_title_words=[\"'structural\", \"engineer'\"], job_title_tf=SparseVector(20, {11: 1.0, 17: 1.0}), job_title_tfidf=SparseVector(20, {11: 1.4034, 17: 2.552}))]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashingTF = HashingTF(inputCol='job_title_words', outputCol='job_title_tf', numFeatures=20)\n",
    "idf       = IDF(inputCol='job_title_tf', outputCol='job_title_tfidf')\n",
    "\n",
    "# putting all the transformers together in a pipeline\n",
    "tokenizer = Tokenizer(inputCol='job_title', outputCol='job_title_words')\n",
    "pipleline = Pipeline(stages=[tokenizer, hashingTF, idf])\n",
    "\n",
    "transformed_employees = pipleline.fit(employees).transform(employees)\n",
    "transformed_employees.select('job_title', 'job_title_words', 'job_title_tf', 'job_title_tfidf').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+----+----+\n",
      "|summary|             col1|col2|col3|\n",
      "+-------+-----------------+----+----+\n",
      "|    min|                1|   1|   1|\n",
      "|   mean|39.74666666666667|38.2|38.6|\n",
      "|    max|               99| 100| 100|\n",
      "|  count|               75|  75|  75|\n",
      "+-------+-----------------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = spark.read.csv('./data/clustering_dataset.csv', header=True, inferSchema=True)\n",
    "dataset.summary('min','mean', 'max','count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+--------------+\n",
      "|col1|col2|col3|      features|\n",
      "+----+----+----+--------------+\n",
      "|   7|   4|   1| [7.0,4.0,1.0]|\n",
      "|   7|   7|   9| [7.0,7.0,9.0]|\n",
      "|   7|   9|   6| [7.0,9.0,6.0]|\n",
      "|   1|   6|   5| [1.0,6.0,5.0]|\n",
      "|   6|   7|   7| [6.0,7.0,7.0]|\n",
      "|   7|   9|   4| [7.0,9.0,4.0]|\n",
      "|   7|  10|   6|[7.0,10.0,6.0]|\n",
      "|   7|   8|   2| [7.0,8.0,2.0]|\n",
      "|   8|   3|   8| [8.0,3.0,8.0]|\n",
      "|   4|  10|   5|[4.0,10.0,5.0]|\n",
      "|   7|   4|   5| [7.0,4.0,5.0]|\n",
      "|   7|   8|   4| [7.0,8.0,4.0]|\n",
      "|   2|   5|   1| [2.0,5.0,1.0]|\n",
      "|   2|   6|   2| [2.0,6.0,2.0]|\n",
      "|   2|   3|   8| [2.0,3.0,8.0]|\n",
      "|   3|   9|   1| [3.0,9.0,1.0]|\n",
      "|   4|   2|   9| [4.0,2.0,9.0]|\n",
      "|   1|   7|   1| [1.0,7.0,1.0]|\n",
      "|   6|   2|   3| [6.0,2.0,3.0]|\n",
      "|   4|   1|   9| [4.0,1.0,9.0]|\n",
      "+----+----+----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# just joining the columns to feature vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['col1', 'col2', 'col3'], outputCol='features')\n",
    "dataset = assembler.transform(dataset)\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means clustering\n",
    "\n",
    "> See [KMeans documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.KMeans.html?highlight=kmeans#pyspark.ml.clustering.KMeans) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([35.88461538, 31.46153846, 34.42307692]),\n",
       " array([80.        , 79.20833333, 78.29166667]),\n",
       " array([5.12, 5.84, 4.84])]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# 3 clusters\n",
    "kmeans = KMeans(featuresCol='features', k=3, seed=1)\n",
    "model = kmeans.fit(dataset)\n",
    "model.clusterCenters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering - BisectingKMeans\n",
    "\n",
    "> See [BisectingKMeans documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.BisectingKMeans.html?highlight=bisectingkmeans#pyspark.ml.clustering.BisectingKMeans) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([5.12, 5.84, 4.84]),\n",
       " array([35.88461538, 31.46153846, 34.42307692]),\n",
       " array([80.        , 79.20833333, 78.29166667])]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "\n",
    "# 3 clusters\n",
    "kmeans = BisectingKMeans(featuresCol='features', k=3, seed=1)\n",
    "model = kmeans.fit(dataset)\n",
    "model.clusterCenters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----------+\n",
      "|_c0|_c1|_c2|_c3|        _c4|\n",
      "+---+---+---+---+-----------+\n",
      "|5.1|3.5|1.4|0.2|Iris-setosa|\n",
      "|4.9|3.0|1.4|0.2|Iris-setosa|\n",
      "|4.7|3.2|1.3|0.2|Iris-setosa|\n",
      "|4.6|3.1|1.5|0.2|Iris-setosa|\n",
      "|5.0|3.6|1.4|0.2|Iris-setosa|\n",
      "|5.4|3.9|1.7|0.4|Iris-setosa|\n",
      "|4.6|3.4|1.4|0.3|Iris-setosa|\n",
      "|5.0|3.4|1.5|0.2|Iris-setosa|\n",
      "|4.4|2.9|1.4|0.2|Iris-setosa|\n",
      "|4.9|3.1|1.5|0.1|Iris-setosa|\n",
      "|5.4|3.7|1.5|0.2|Iris-setosa|\n",
      "|4.8|3.4|1.6|0.2|Iris-setosa|\n",
      "|4.8|3.0|1.4|0.1|Iris-setosa|\n",
      "|4.3|3.0|1.1|0.1|Iris-setosa|\n",
      "|5.8|4.0|1.2|0.2|Iris-setosa|\n",
      "|5.7|4.4|1.5|0.4|Iris-setosa|\n",
      "|5.4|3.9|1.3|0.4|Iris-setosa|\n",
      "|5.1|3.5|1.4|0.3|Iris-setosa|\n",
      "|5.7|3.8|1.7|0.3|Iris-setosa|\n",
      "|5.1|3.8|1.5|0.3|Iris-setosa|\n",
      "+---+---+---+---+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = spark.read.csv('./data/iris.csv', inferSchema=True)\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "> **Iris** [Dataset Link](https://archive.ics.uci.edu/ml/datasets/iris).\n",
    "\n",
    "- Creating a pipeline of transformations [RenameColumns -> JoinFeatureColumns -> EncodeTargetLabels -> splitTrainTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a custom transformer to rename the columns to friendly names\n",
    "from pyspark.ml import Transformer\n",
    "\n",
    "class ColumnRenamer(Transformer):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, df):\n",
    "        return df.selectExpr(\n",
    "                            '_c0 AS sepal_length',\n",
    "                            '_c1 AS sepal_width',\n",
    "                            '_c2 AS petal_length',\n",
    "                            '_c3 AS petal_width',\n",
    "                            '_c4 AS species')\n",
    "column_renamer = ColumnRenamer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a transformer to join columns to feature vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "string_indexer = StringIndexer(inputCol='species', outputCol='class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a custom transformer to split the dataframe into training and test data\n",
    "\n",
    "class TrainTestDataSplitter(Transformer):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, df):\n",
    "        return df.randomSplit([0.7, 0.3], 1)\n",
    "train_test_data_splitter = TrainTestDataSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Row(sepal_length=4.3, sepal_width=3.0, petal_length=1.1, petal_width=0.1, species='Iris-setosa', features=DenseVector([4.3, 3.0, 1.1, 0.1]), class=0.0)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assembling the pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[column_renamer, assembler, string_indexer, train_test_data_splitter])\n",
    "train_df, test_df = pipeline.fit(dataset).transform(dataset)\n",
    "display(\n",
    "    dataset.count(),\n",
    "    train_df.count(),\n",
    "    test_df.count(),\n",
    "    train_df.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def train_and_evaluate_classification_model(classifier, df_train, df_test) -> float:\n",
    "    model = classifier.fit(df_train)\n",
    "\n",
    "    predictions = model.transform(df_test)\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='class', predictionCol='prediction', metricName='accuracy')\n",
    "    acc = evaluator.evaluate(predictions)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "\n",
    "> Naive Bayes Classifiers. See [NaiveBayes documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.NaiveBayes.html?highlight=naivebayes#pyspark.ml.classification.NaiveBayes) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "naive = NaiveBayes(featuresCol='features', labelCol='class', predictionCol='prediction')\n",
    "train_and_evaluate_classification_model(naive, train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilayer Perceptron Classifier\n",
    "\n",
    "> Classifier trainer based on the Multilayer Perceptron. Each layer has sigmoid activation function, output layer has softmax. See [MultilayerPerceptronClassifier documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.MultilayerPerceptronClassifier.html?highlight=multilayer%20perceptron%20classifier#pyspark.ml.classification.MultilayerPerceptronClassifier) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "# 4 hidden layers: 4 (features), 4, 4, 3 (classes)\n",
    "mlp = MultilayerPerceptronClassifier(layers=[4, 4, 4, 3], featuresCol='features', labelCol='class', predictionCol='prediction')\n",
    "train_and_evaluate_classification_model(mlp, train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier\n",
    "\n",
    "> Decision tree learning algorithm for classification. See [DecisionTreeClassifier documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.DecisionTreeClassifier.html?highlight=decision%20trees%20classifiers) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.925"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "tree = DecisionTreeClassifier(featuresCol='features', labelCol='class', predictionCol='prediction')\n",
    "train_and_evaluate_classification_model(tree, train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset\n",
    "\n",
    "> **Red-Wine Quality** [Dataset Link](https://archive.ics.uci.edu/ml/datasets/wine+quality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fixed acidity: double (nullable = true)\n",
      " |-- volatile acidity: double (nullable = true)\n",
      " |-- citric acid: double (nullable = true)\n",
      " |-- residual sugar: double (nullable = true)\n",
      " |-- chlorides: double (nullable = true)\n",
      " |-- free sulfur dioxide: double (nullable = true)\n",
      " |-- total sulfur dioxide: double (nullable = true)\n",
      " |-- density: double (nullable = true)\n",
      " |-- pH: double (nullable = true)\n",
      " |-- sulphates: double (nullable = true)\n",
      " |-- alcohol: double (nullable = true)\n",
      " |-- quality: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('./data/winequality-red.csv', header=True, inferSchema=True, sep=';')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking for columns with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_null_values = [column for column in df.columns if df.filter(f\"'{column}' is NULL\").count() > 0]\n",
    "columns_with_null_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysing columns statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|summary|fixed acidity|\n",
      "+-------+-------------+\n",
      "|  count|       1599.0|\n",
      "|   mean|         8.32|\n",
      "| stddev|         1.74|\n",
      "|    min|          4.6|\n",
      "|    max|         15.9|\n",
      "+-------+-------------+\n",
      "\n",
      "+-------+----------------+\n",
      "|summary|volatile acidity|\n",
      "+-------+----------------+\n",
      "|  count|          1599.0|\n",
      "|   mean|            0.53|\n",
      "| stddev|            0.18|\n",
      "|    min|            0.12|\n",
      "|    max|            1.58|\n",
      "+-------+----------------+\n",
      "\n",
      "+-------+-----------+\n",
      "|summary|citric acid|\n",
      "+-------+-----------+\n",
      "|  count|     1599.0|\n",
      "|   mean|       0.27|\n",
      "| stddev|       0.19|\n",
      "|    min|        0.0|\n",
      "|    max|        1.0|\n",
      "+-------+-----------+\n",
      "\n",
      "+-------+--------------+\n",
      "|summary|residual sugar|\n",
      "+-------+--------------+\n",
      "|  count|        1599.0|\n",
      "|   mean|          2.54|\n",
      "| stddev|          1.41|\n",
      "|    min|           0.9|\n",
      "|    max|          15.5|\n",
      "+-------+--------------+\n",
      "\n",
      "+-------+---------+\n",
      "|summary|chlorides|\n",
      "+-------+---------+\n",
      "|  count|   1599.0|\n",
      "|   mean|     0.09|\n",
      "| stddev|     0.05|\n",
      "|    min|     0.01|\n",
      "|    max|     0.61|\n",
      "+-------+---------+\n",
      "\n",
      "+-------+-------------------+\n",
      "|summary|free sulfur dioxide|\n",
      "+-------+-------------------+\n",
      "|  count|             1599.0|\n",
      "|   mean|              15.87|\n",
      "| stddev|              10.46|\n",
      "|    min|                1.0|\n",
      "|    max|               72.0|\n",
      "+-------+-------------------+\n",
      "\n",
      "+-------+--------------------+\n",
      "|summary|total sulfur dioxide|\n",
      "+-------+--------------------+\n",
      "|  count|              1599.0|\n",
      "|   mean|               46.47|\n",
      "| stddev|                32.9|\n",
      "|    min|                 6.0|\n",
      "|    max|               289.0|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+-------+\n",
      "|summary|density|\n",
      "+-------+-------+\n",
      "|  count| 1599.0|\n",
      "|   mean|    1.0|\n",
      "| stddev|    0.0|\n",
      "|    min|   0.99|\n",
      "|    max|    1.0|\n",
      "+-------+-------+\n",
      "\n",
      "+-------+------+\n",
      "|summary|    pH|\n",
      "+-------+------+\n",
      "|  count|1599.0|\n",
      "|   mean|  3.31|\n",
      "| stddev|  0.15|\n",
      "|    min|  2.74|\n",
      "|    max|  4.01|\n",
      "+-------+------+\n",
      "\n",
      "+-------+---------+\n",
      "|summary|sulphates|\n",
      "+-------+---------+\n",
      "|  count|   1599.0|\n",
      "|   mean|     0.66|\n",
      "| stddev|     0.17|\n",
      "|    min|     0.33|\n",
      "|    max|      2.0|\n",
      "+-------+---------+\n",
      "\n",
      "+-------+-------+\n",
      "|summary|alcohol|\n",
      "+-------+-------+\n",
      "|  count| 1599.0|\n",
      "|   mean|  10.42|\n",
      "| stddev|   1.07|\n",
      "|    min|    8.4|\n",
      "|    max|   14.9|\n",
      "+-------+-------+\n",
      "\n",
      "+-------+-------+\n",
      "|summary|quality|\n",
      "+-------+-------+\n",
      "|  count| 1599.0|\n",
      "|   mean|   5.64|\n",
      "| stddev|   0.81|\n",
      "|    min|    3.0|\n",
      "|    max|    8.0|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "for col in df.columns:\n",
    "    # round column values to 2 decimal places\n",
    "    df.select(col).describe().select('summary', round(col, 2).alias(col)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            features|quality|\n",
      "+--------------------+-------+\n",
      "|[7.4,0.7,0.0,1.9,...|      5|\n",
      "|[7.8,0.88,0.0,2.6...|      5|\n",
      "|[7.8,0.76,0.04,2....|      5|\n",
      "|[11.2,0.28,0.56,1...|      6|\n",
      "|[7.4,0.7,0.0,1.9,...|      5|\n",
      "|[7.4,0.66,0.0,1.8...|      5|\n",
      "|[7.9,0.6,0.06,1.6...|      5|\n",
      "|[7.3,0.65,0.0,1.2...|      7|\n",
      "|[7.8,0.58,0.02,2....|      7|\n",
      "|[7.5,0.5,0.36,6.1...|      5|\n",
      "|[6.7,0.58,0.08,1....|      5|\n",
      "|[7.5,0.5,0.36,6.1...|      5|\n",
      "|[5.6,0.615,0.0,1....|      5|\n",
      "|[7.8,0.61,0.29,1....|      5|\n",
      "|[8.9,0.62,0.18,3....|      5|\n",
      "|[8.9,0.62,0.19,3....|      5|\n",
      "|[8.5,0.28,0.56,1....|      7|\n",
      "|[8.1,0.56,0.28,1....|      5|\n",
      "|[7.4,0.59,0.08,4....|      4|\n",
      "|[7.9,0.32,0.51,1....|      6|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining feature columns\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "target_column = 'quality'\n",
    "features = df.columns\n",
    "features.remove(target_column)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features, outputCol='features')\n",
    "df = assembler.transform(df).select('features', target_column)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            features|quality|\n",
      "+--------------------+-------+\n",
      "|[-0.5281943702487...|      5|\n",
      "|[-0.2984540647668...|      5|\n",
      "|[-0.2984540647668...|      5|\n",
      "|[1.65433853182897...|      6|\n",
      "|[-0.5281943702487...|      5|\n",
      "|[-0.5281943702487...|      5|\n",
      "|[-0.2410189883963...|      5|\n",
      "|[-0.5856294466191...|      7|\n",
      "|[-0.2984540647668...|      7|\n",
      "|[-0.4707592938782...|      5|\n",
      "|[-0.9302399048419...|      5|\n",
      "|[-0.4707592938782...|      5|\n",
      "|[-1.5620257449170...|      5|\n",
      "|[-0.2984540647668...|      5|\n",
      "|[0.33333177530827...|      5|\n",
      "|[0.33333177530827...|      5|\n",
      "|[0.10359146982641...|      7|\n",
      "|[-0.1261488356554...|      5|\n",
      "|[-0.5281943702487...|      4|\n",
      "|[-0.2410189883963...|      6|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standardizing the data\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql.functions import column\n",
    "\n",
    "standardizer = StandardScaler(inputCol='features', outputCol='scaled_features', withStd=True, withMean=True)\n",
    "df = standardizer.fit(df).transform(df).select(column('scaled_features').alias('features'), target_column)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and test data\n",
    "\n",
    "df_train, df_test = df.randomSplit([0.7, 0.3], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "def train_and_evaluate_regression_model(regressor, df_train, df_test) -> tuple[float, float]:\n",
    "    model = regressor.fit(df_train)\n",
    "\n",
    "    predictions = model.transform(df_test)\n",
    "\n",
    "    # Evaluate the model aproximation to real data\n",
    "    reg_evaluator = RegressionEvaluator(labelCol='quality', predictionCol='prediction', metricName='rmse')\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "    # Evaluating the integer value prediction\n",
    "    class_evaluator = MulticlassClassificationEvaluator(labelCol='quality', predictionCol='prediction', metricName='accuracy')\n",
    "    acc = class_evaluator.evaluate(predictions.select(round('prediction', 0).alias('prediction'), 'quality'))\n",
    "    return rmse, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "\n",
    "> The learning objective is to minimize the specified loss function, with regularization. See [LinearRegression documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegression.html?highlight=linear%20regression#pyspark.ml.regression.LinearRegression) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6434870482395905, 0.558695652173913)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol='features', labelCol='quality', predictionCol='prediction')\n",
    "train_and_evaluate_regression_model(lr, df_train, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor\n",
    "\n",
    "> Decision tree learning algorithm for regression. See [DecisionTreeRegressor documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.DecisionTreeRegressor.html#pyspark.ml.regression.DecisionTreeRegressor) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6660182397865323, 0.591304347826087)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(featuresCol='features', labelCol='quality', predictionCol='prediction')\n",
    "train_and_evaluate_regression_model(tree, df_train, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient-Boosted Trees (GBTs)\n",
    "\n",
    "> Gradient-Boosted Trees (GBTs) learning algorithm. See [GBTRegressor documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.GBTRegressor.html?highlight=gbt%20regression) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6535988383824604, 0.5978260869565217)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='quality', predictionCol='prediction')\n",
    "train_and_evaluate_regression_model(gbt, df_train, df_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "91e1978be154cf5421f2f21f3417c39a219e9c14846f040d23c9fcd9f86222c3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
